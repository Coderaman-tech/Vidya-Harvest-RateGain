
# Problem Statement

Create an application that systematically explores
"https://rategain.com/blog," traverses multiple pages,
and gathers essential data from highlighted blog posts.
The application should adeptly capture blog titles,
publication dates, image URLs, and likes counts. To
facilitate seamless analysis, ensure the extracted data is
efficiently organized and saved in either Excel or CSV
format.
# Vidya Harvest

Vidya Harvest is a Python-based web scraping tool designed to extract information from a https://rategain.com/blog website. This project utilizes the Selenium library for automated browser interactions and provides a simple and efficient way to gather data from multiple pages.

## Features

- **Dynamic Scraping:** Navigate through multiple pages to comprehensively gather data.
- **Data Extraction:** Extract blog titles, publication dates, image URLs, and likes counts.
- **CSV Storage:** Organize and save the extracted data in a CSV file for easy analysis.

## Getting Started

1. **Installation:**
   ```bash
   pip install selenium
## Usage:

1. Open the Jupyter Notebook file (scrape.ipynb) in your Jupyter environment.
Ensure that you have the necessary libraries installed.
Configuration:

2. Adjust the implicit wait time based on your network speed and website responsiveness.
Run the Notebook:

3. Execute the notebook cell by cell to scrape data from the  https://rategain.com/blog website.
## Badges

Add badges from somewhere like: [shields.io](https://shields.io/)

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
[![GPLv3 License](https://img.shields.io/badge/License-GPL%20v3-yellow.svg)](https://opensource.org/licenses/)
[![AGPL License](https://img.shields.io/badge/license-AGPL-blue.svg)](http://www.gnu.org/licenses/agpl-3.0)

